{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to run tl-rnn stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "    from util import *\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    from model import tlrnn\n",
    "    from dataPrep import dataprep\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define directory where you stored the data\n",
    "\"\"\"\n",
    "directory = \"~/tl_rnn/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 31.99 ms\n",
      "Type conversion took: 12.88 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.27 ms\n",
      "Type conversion took: 12.34 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.69 ms\n",
      "Type conversion took: 13.67 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.46 ms\n",
      "Type conversion took: 14.54 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.22 ms\n",
      "Type conversion took: 14.28 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.26 ms\n",
      "Type conversion took: 14.21 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.13 ms\n",
      "Type conversion took: 14.29 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.68 ms\n",
      "Type conversion took: 13.55 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.45 ms\n",
      "Type conversion took: 15.44 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 12.98 ms\n",
      "Type conversion took: 16.18 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.99 ms\n",
      "Type conversion took: 15.14 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 2.74 ms\n",
      "Type conversion took: 4.09 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "read training dataset and get rid of NA's\n",
    "\"\"\"\n",
    "df = pd.read_csv(directory + \"sequential_data_train.csv\", verbose=True)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLUMNS  ['user_ID', 'sequence_ID', 'eventData_1', 'eventData_2', 'eventData_3']\n",
      "SEQ_LEN_MAX\u001b[95m 100\n",
      "TOP 20 Categories of all Covariates ['cat_45', 'cat_73', 'cat_44', 'cat_26', 'cat_90', 'cat_213', 'cat_160', 'cat_69', 'cat_244', 'cat_232', 'cat_752', 'cat_133', 'cat_506', 'cat_672', 'cat_425', 'cat_403', 'cat_235', 'cat_390', 'cat_83', 'cat_547']\n",
      "COVARIATES_ALL\u001b[95m ['eventData_1', 'eventData_2', 'eventData_3']\n",
      "SAMPLES PER USER 128\n",
      "SEQUENCE LENGTH MAX 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create dataprep object. in this object all hyperparameters for dataprocessing and sampling are stored\n",
    "please double check all parameters in config.py before running it!!!\n",
    "\"\"\"\n",
    "\n",
    "dataobj = dataprep(dataframe=df, id_name=\"user_ID\", sequence_id = \"sequence_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe -> tensor ...\n",
      "--------------------------------------------------\n",
      "15/02/2022 19:21:03\n",
      "0 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:22:20\n",
      "1000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:23:32\n",
      "2000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:24:32\n",
      "3000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:25:25\n",
      "4000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:26:05\n",
      "5000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:26:38\n",
      "6000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:27:04\n",
      "7000 of 9000 users \n",
      "--------------------------------------------------\n",
      "15/02/2022 19:27:26\n",
      "8000 of 9000 users \n",
      "tensor builder finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "        move data from original dataframe into lists of arrays,\n",
    "        encode them properly: categories with a ranked based encoding\n",
    "        one list entry per user array, with [timeframe index (e.g. nth week), seq_len(e.g. 1000), feature]\n",
    "        time order conserved (t-1,t0,t+1)\n",
    "\n",
    "        params: only members coming from dataprep init\n",
    "\n",
    "        input:  columnwise dataframe read by dataframe() init\n",
    "\n",
    "        return: list with each element being a user nested with another list of single array sequences:\n",
    "                [user][array[timestep, features]]\n",
    "\"\"\"\n",
    "\n",
    "dataobj.preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eventData_1': 34808, 'eventData_2': 26, 'eventData_3': 33}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluate cardinalities of input values\n",
    "this numbers will be taken into account to dimension the neural network model\n",
    "\"\"\"\n",
    "\n",
    "dataobj.build_cardinality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training with the training data, let's prepare the holdout (test) data-set to evaluate the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read column-like TEST dataframe and get rid of NA's\n",
    "\"\"\"\n",
    "\n",
    "df_test = pd.read_csv(directory + \"sequential_data_test.csv\")\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLUMNS  ['user_ID', 'sequence_ID', 'eventData_1', 'eventData_2', 'eventData_3']\n",
      "SEQ_LEN_MAX\u001b[95m 100\n",
      "TOP 20 Categories of all Covariates ['cat_45', 'cat_73', 'cat_44', 'cat_90', 'cat_26', 'cat_69', 'cat_213', 'cat_232', 'cat_752', 'cat_160', 'cat_244', 'cat_251', 'cat_390', 'cat_672', 'cat_547', 'cat_324', 'cat_425', 'cat_506', 'cat_133', 'cat_402']\n",
      "COVARIATES_ALL\u001b[95m ['eventData_1', 'eventData_2', 'eventData_3']\n",
      "SAMPLES PER USER 128\n",
      "SEQUENCE LENGTH MAX 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create dataprep object for test data\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test = dataprep(dataframe=df_test, id_name=\"user_ID\", sequence_id = \"sequence_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VERY IMPORTANT!!\n",
    "\n",
    "Overwrite the feature encodings (rank-based) with the ones from the training set\n",
    "Otherwise training and test are encoded differently and the predicitve power gets bad\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test.ranks = dataobj.ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe -> tensor ...\n",
      "--------------------------------------------------\n",
      "15/02/2022 19:28:05\n",
      "0 of 1000 users \n",
      "tensor builder finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run preprocessor and bring datframe into propper list-like data structure for sampler\n",
    "\"\"\"\n",
    "dataobj_test.preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplets per user:  8\n",
      "0 of 1232 users \r\n",
      "1000 of 1232 users \r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample triplets from holdout (test) set to test re-identification score\n",
    "Notice: In training mode, the triplets are sampled online during training,\n",
    "for each epoch a new set of triplets\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test.sampler(triplets_per_user=8, users_per_file=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'inputs' and 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-31db81861439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmy_tlrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtlrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmy_tlrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tl_rnn/src/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cells, dataobj)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_last_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTAKE_LAST_ACTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcells\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "define cells and build tl-rnn keras model\n",
    "put in dataobj, where also the pre-processed data-set\n",
    "is included for \"online\" generator sampling\n",
    "\"\"\"\n",
    "\n",
    "my_tlrnn = tlrnn(cells=128, dataobj=dataobj)\n",
    "my_tlrnn.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-68601943eabe>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-68601943eabe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python3 -c 'import tensorflow as tf; print(tf.__version__)'\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplets per user:  2\n",
      "0 of 10000 users \n",
      "1000 of 10000 users \n",
      "2000 of 10000 users \n",
      "3000 of 10000 users \n",
      "4000 of 10000 users \n",
      "5000 of 10000 users \n",
      "6000 of 10000 users \n",
      "7000 of 10000 users \n",
      "8000 of 10000 users \n",
      "9000 of 10000 users \n"
     ]
    }
   ],
   "source": [
    "dataobj.sampler(triplets_per_user=2, users_per_file=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVARIATES:  {'eventData_1': 35719, 'eventData_2': 25, 'eventData_3': 32}\n",
      "SEQUENCE LEN MAX 100\n",
      "CELLS 128\n",
      "PATIENCE 1\n",
      "BATCH SIZE 64\n",
      "BATCHES PER USER 2\n",
      "OPTIMIZER <keras.optimizers.Adam object at 0x7f3fcf675c88>\n"
     ]
    }
   ],
   "source": [
    "my_tlrnn = tlrnn(cells=128, dataobj=dataobj)\n",
    "my_tlrnn.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split training and validation set\n",
      "length training set:  9500\n",
      "length validation set:  500\n",
      "compile model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 1, 42)   1500198     input_1[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 1, 9)    225         input_2[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 1, 9)    288         input_3[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_00 (Reshape)            (None, 100, 42)      0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_01 (Reshape)            (None, 100, 9)       0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_02 (Reshape)            (None, 100, 9)       0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_03 (Reshape)            (None, 100, 42)      0           time_distributed_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_04 (Reshape)            (None, 100, 9)       0           time_distributed_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_05 (Reshape)            (None, 100, 9)       0           time_distributed_3[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_06 (Reshape)            (None, 100, 42)      0           time_distributed_1[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_07 (Reshape)            (None, 100, 9)       0           time_distributed_2[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_08 (Reshape)            (None, 100, 9)       0           time_distributed_3[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 60)      0           reshape_00[0][0]                 \n",
      "                                                                 reshape_01[0][0]                 \n",
      "                                                                 reshape_02[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100, 60)      0           reshape_03[0][0]                 \n",
      "                                                                 reshape_04[0][0]                 \n",
      "                                                                 reshape_05[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 60)      0           reshape_06[0][0]                 \n",
      "                                                                 reshape_07[0][0]                 \n",
      "                                                                 reshape_08[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_gpu (CuDNNLSTM)            [(None, 100, 128), ( 97280       concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           lstm_gpu[0][2]                   \n",
      "                                                                 lstm_gpu[1][2]                   \n",
      "                                                                 lstm_gpu[2][2]                   \n",
      "==================================================================================================\n",
      "Total params: 1,597,991\n",
      "Trainable params: 1,597,991\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "valid anchor users in validation set (at least two sequences):  1000\n",
      "valid anchor users in training set (at least two sequences):  19000\n",
      "Epoch 1/12\n",
      "18991/19000 [============================>.] - ETA: 0s - loss: 0.4059 number of users skipped:  0\n",
      "18997/19000 [============================>.] - ETA: 0s - loss: 0.4058 number of users skipped:  0\n",
      "19000/19000 [==============================] - 510s 27ms/step - loss: 0.4058 - val_loss: 0.3040\n",
      "score L1:  0.89674453125\n",
      "Epoch 2/12\n",
      "18990/19000 [============================>.] - ETA: 0s - loss: 0.2352 number of users skipped:  0\n",
      "18999/19000 [============================>.] - ETA: 0s - loss: 0.2351 number of users skipped:  0\n",
      "19000/19000 [==============================] - 486s 26ms/step - loss: 0.2351 - val_loss: 0.2641\n",
      "score L1:  0.9256015625\n",
      "Epoch 3/12\n",
      "18991/19000 [============================>.] - ETA: 0s - loss: 0.1717 number of users skipped:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18997/19000 [============================>.] - ETA: 0s - loss: 0.1716 number of users skipped:  0\n",
      "19000/19000 [==============================] - 487s 26ms/step - loss: 0.1716 - val_loss: 0.2360\n",
      "score L1:  0.9426640625\n",
      "Epoch 4/12\n",
      "18990/19000 [============================>.] - ETA: 0s - loss: 0.1329 number of users skipped:  0\n",
      "18999/19000 [============================>.] - ETA: 0s - loss: 0.1328 number of users skipped:  0\n",
      "19000/19000 [==============================] - 484s 25ms/step - loss: 0.1328 - val_loss: 0.2261\n",
      "score L1:  0.95375546875\n",
      "Epoch 5/12\n",
      "18991/19000 [============================>.] - ETA: 0s - loss: 0.1079 number of users skipped:  0\n",
      "18997/19000 [============================>.] - ETA: 0s - loss: 0.1079 number of users skipped:  0\n",
      "19000/19000 [==============================] - 486s 26ms/step - loss: 0.1079 - val_loss: 0.2135\n",
      "score L1:  0.960203125\n",
      "Epoch 6/12\n",
      "18991/19000 [============================>.] - ETA: 0s - loss: 0.0919 number of users skipped:  0\n",
      "18997/19000 [============================>.] - ETA: 0s - loss: 0.0919 number of users skipped:  0\n",
      "19000/19000 [==============================] - 487s 26ms/step - loss: 0.0919 - val_loss: 0.2129\n",
      "score L1:  0.9657203125\n",
      "Epoch 7/12\n",
      "18992/19000 [============================>.] - ETA: 0s - loss: 0.0809 number of users skipped:  0\n",
      "18998/19000 [============================>.] - ETA: 0s - loss: 0.0809 number of users skipped:  0\n",
      "19000/19000 [==============================] - 486s 26ms/step - loss: 0.0809 - val_loss: 0.2143\n",
      "score L1:  0.9692328125\n",
      "Epoch 00007: early stopping\n",
      "training complete!\n"
     ]
    }
   ],
   "source": [
    "# train generator version\n",
    "\n",
    "alpha=1.0\n",
    "\n",
    "run_name = \"test_insta_generator\"\n",
    "\n",
    "my_tlrnn.train_generator(run_name=run_name, alpha=alpha, test_set=dataobj.triplet_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_array = np.zeros((len(dataobj.cov_number)), dtype='int32')\n",
    "\n",
    "for user_list in dataobj.data_list:\n",
    "    for seq in user_list:\n",
    "        for feature in dataobj.cov_number:\n",
    "            if max(seq[:, feature]) > max_array[feature]:\n",
    "                max_array[feature] = max(seq[:, feature])\n",
    "\n",
    "max_array = max_array + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVARIATES_ALL = [\"eventData_1\", \"eventData_2\", \"eventData_3\"]\n",
    "\n",
    "COVARIATES_TO_TRANSLATE = [\"eventData_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "i = 0\n",
    "while (i < len(COVARIATES_ALL)):\n",
    "    if (COVARIATES_TO_TRANSLATE.count(COVARIATES_ALL[i]) > 0):\n",
    "        res.append(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [1,2,3,4]\n",
    "list2 = [5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3%1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1804     9    11]\n",
      " [ 2962     9    11]\n",
      " [  164     9    11]\n",
      " [ 3472     9    11]\n",
      " [ 1356     9    11]\n",
      " [ 1078     9    11]\n",
      " [29333     9    11]\n",
      " [11025     9    11]\n",
      " [15511     9    11]\n",
      " [ 3317     9    11]\n",
      " [25435     9    11]\n",
      " [25445     9    11]\n",
      " [26191     9    11]\n",
      " [ 6018     9    11]\n",
      " [26405     9    11]]\n",
      "---------------\n",
      "******************\n",
      "[ 1804  2962   164  3472  1356  1078 29333 11025 15511  3317 25435 25445\n",
      " 26191  6018 26405]\n",
      "******************\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a112a709e659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"******************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0mmax_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_array' is not defined"
     ]
    }
   ],
   "source": [
    "for user_list in dataobj.data_list:\n",
    "    for seq in user_list:\n",
    "        print(seq)\n",
    "        print(\"---------------\")\n",
    "        for feature in range(3):\n",
    "            print(\"******************\")\n",
    "            print(seq[:, feature])\n",
    "            print(\"******************\")\n",
    "            if max(seq[:, feature]) > max_array[feature]:\n",
    "                max_array[feature] = max(seq[:, feature])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu_env] *",
   "language": "python",
   "name": "conda-env-gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook to run tl-rnn stepwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "    from util import *\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    import random\n",
    "    from model import tlrnn\n",
    "    from dataPrep import dataprep\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "define directory where you stored the data\n",
    "\"\"\"\n",
    "directory = \"~/tl_rnn/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization took: 21.90 ms\n",
      "Type conversion took: 13.64 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.41 ms\n",
      "Type conversion took: 12.72 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.84 ms\n",
      "Type conversion took: 13.61 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.33 ms\n",
      "Type conversion took: 14.40 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.32 ms\n",
      "Type conversion took: 14.34 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.32 ms\n",
      "Type conversion took: 14.31 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.05 ms\n",
      "Type conversion took: 15.03 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 9.56 ms\n",
      "Type conversion took: 13.78 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.26 ms\n",
      "Type conversion took: 16.10 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 11.26 ms\n",
      "Type conversion took: 15.39 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 10.27 ms\n",
      "Type conversion took: 15.23 ms\n",
      "Parser memory cleanup took: 0.00 ms\n",
      "Tokenization took: 2.74 ms\n",
      "Type conversion took: 4.19 ms\n",
      "Parser memory cleanup took: 0.00 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "read training dataset and get rid of NA's\n",
    "\"\"\"\n",
    "df = pd.read_csv(directory + \"sequential_data_train.csv\", verbose=True)\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLUMNS  ['user_ID', 'sequence_ID', 'eventData_1', 'eventData_2', 'eventData_3']\n",
      "SEQ_LEN_MAX\u001b[95m 100\n",
      "TOP 20 Categories of all Covariates ['cat_45', 'cat_73', 'cat_44', 'cat_26', 'cat_90', 'cat_213', 'cat_160', 'cat_69', 'cat_244', 'cat_232', 'cat_752', 'cat_133', 'cat_506', 'cat_672', 'cat_425', 'cat_403', 'cat_235', 'cat_390', 'cat_83', 'cat_547']\n",
      "COVARIATES_ALL\u001b[95m ['eventData_1', 'eventData_2', 'eventData_3']\n",
      "SAMPLES PER USER 128\n",
      "SEQUENCE LENGTH MAX 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create dataprep object. in this object all hyperparameters for dataprocessing and sampling are stored\n",
    "please double check all parameters in config.py before running it!!!\n",
    "\"\"\"\n",
    "\n",
    "dataobj = dataprep(dataframe=df, id_name=\"user_ID\", sequence_id = \"sequence_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe -> tensor ...\n",
      "--------------------------------------------------\n",
      "16/02/2022 13:55:41\n",
      "0 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 13:56:58\n",
      "1000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 13:58:09\n",
      "2000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 13:59:08\n",
      "3000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 13:59:57\n",
      "4000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 14:00:37\n",
      "5000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 14:01:09\n",
      "6000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 14:01:35\n",
      "7000 of 9000 users \n",
      "--------------------------------------------------\n",
      "16/02/2022 14:01:56\n",
      "8000 of 9000 users \n",
      "tensor builder finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "        move data from original dataframe into lists of arrays,\n",
    "        encode them properly: categories with a ranked based encoding\n",
    "        one list entry per user array, with [timeframe index (e.g. nth week), seq_len(e.g. 1000), feature]\n",
    "        time order conserved (t-1,t0,t+1)\n",
    "\n",
    "        params: only members coming from dataprep init\n",
    "\n",
    "        input:  columnwise dataframe read by dataframe() init\n",
    "\n",
    "        return: list with each element being a user nested with another list of single array sequences:\n",
    "                [user][array[timestep, features]]\n",
    "\"\"\"\n",
    "\n",
    "dataobj.preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eventData_1': 34808, 'eventData_2': 26, 'eventData_3': 33}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "evaluate cardinalities of input values\n",
    "this numbers will be taken into account to dimension the neural network model\n",
    "\"\"\"\n",
    "\n",
    "dataobj.build_cardinality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training with the training data, let's prepare the holdout (test) data-set to evaluate the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read column-like TEST dataframe and get rid of NA's\n",
    "\"\"\"\n",
    "\n",
    "df_test = pd.read_csv(directory + \"sequential_data_test.csv\")\n",
    "df_test = df_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATAFRAME COLUMNS  ['user_ID', 'sequence_ID', 'eventData_1', 'eventData_2', 'eventData_3']\n",
      "SEQ_LEN_MAX\u001b[95m 100\n",
      "TOP 20 Categories of all Covariates ['cat_45', 'cat_73', 'cat_44', 'cat_90', 'cat_26', 'cat_69', 'cat_213', 'cat_232', 'cat_752', 'cat_160', 'cat_244', 'cat_251', 'cat_390', 'cat_672', 'cat_547', 'cat_324', 'cat_425', 'cat_506', 'cat_133', 'cat_402']\n",
      "COVARIATES_ALL\u001b[95m ['eventData_1', 'eventData_2', 'eventData_3']\n",
      "SAMPLES PER USER 128\n",
      "SEQUENCE LENGTH MAX 100\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create dataprep object for test data\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test = dataprep(dataframe=df_test, id_name=\"user_ID\", sequence_id = \"sequence_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VERY IMPORTANT!!\n",
    "\n",
    "Overwrite the feature encodings (rank-based) with the ones from the training set\n",
    "Otherwise training and test are encoded differently and the predicitve power gets bad\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test.ranks = dataobj.ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe -> tensor ...\n",
      "--------------------------------------------------\n",
      "16/02/2022 14:02:13\n",
      "0 of 1000 users \n",
      "tensor builder finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run preprocessor and bring datframe into propper list-like data structure for sampler\n",
    "\"\"\"\n",
    "dataobj_test.preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triplets per user:  8\n",
      "0 of 1000 users \r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample triplets from holdout (test) set to test re-identification score\n",
    "Notice: In training mode, the triplets are sampled online during training,\n",
    "for each epoch a new set of triplets\n",
    "\"\"\"\n",
    "\n",
    "dataobj_test.sampler(triplets_per_user=8, users_per_file=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVARIATES:  {'eventData_1': 34808, 'eventData_2': 26, 'eventData_3': 33}\n",
      "SEQUENCE LEN MAX 100\n",
      "CELLS 128\n",
      "PATIENCE 4\n",
      "BATCH SIZE 64\n",
      "BATCHES PER USER 2\n",
      "OPTIMIZER <keras.optimizers.Adam object at 0x7f2e3dcca2b0>\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "define cells and build tl-rnn keras model\n",
    "put in dataobj, where also the pre-processed data-set\n",
    "is included for \"online\" generator sampling\n",
    "\"\"\"\n",
    "\n",
    "my_tlrnn = tlrnn(cells=128, dataobj=dataobj)\n",
    "my_tlrnn.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "alpha: seperation strength (regularizer) for triplet seperation\n",
    "too much: model doesn't generalize well\n",
    "too weak: model doesn't discriminate well\n",
    "start with alpha=1.0 and work upwards\n",
    "\"\"\"\n",
    "\n",
    "alpha=1.0\n",
    "run_name = \"my_run_01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split training and validation set\n",
      "length training set:  8550\n",
      "length validation set:  450\n",
      "compile model...\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 100, 1, 42)   1461936     input_1[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 100, 1, 9)    234         input_2[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 100, 1, 9)    297         input_3[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_00 (Reshape)            (None, 100, 42)      0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_01 (Reshape)            (None, 100, 9)       0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_02 (Reshape)            (None, 100, 9)       0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_03 (Reshape)            (None, 100, 42)      0           time_distributed_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_04 (Reshape)            (None, 100, 9)       0           time_distributed_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_05 (Reshape)            (None, 100, 9)       0           time_distributed_3[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_06 (Reshape)            (None, 100, 42)      0           time_distributed_1[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_07 (Reshape)            (None, 100, 9)       0           time_distributed_2[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "reshape_08 (Reshape)            (None, 100, 9)       0           time_distributed_3[2][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 60)      0           reshape_00[0][0]                 \n",
      "                                                                 reshape_01[0][0]                 \n",
      "                                                                 reshape_02[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 100, 60)      0           reshape_03[0][0]                 \n",
      "                                                                 reshape_04[0][0]                 \n",
      "                                                                 reshape_05[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 100, 60)      0           reshape_06[0][0]                 \n",
      "                                                                 reshape_07[0][0]                 \n",
      "                                                                 reshape_08[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (CuDNNLSTM)              [(None, 100, 128), ( 97280       concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           lstm_1[0][2]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "==================================================================================================\n",
      "Total params: 1,559,747\n",
      "Trainable params: 1,559,747\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "valid anchor users in validation set (at least two sequences):  900\n",
      "valid anchor users in training set (at least two sequences):  17100\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.4212 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.4211 number of users skipped:  0\n",
      "17100/17100 [==============================] - 361s 21ms/step - loss: 0.4211 - val_loss: 0.3201\n",
      "WARNING:tensorflow:From /home/stefan/miniconda3/envs/gpu_env/lib/python3.6/site-packages/keras/callbacks.py:869: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "score L1:  0.8635\n",
      "Epoch 2/12\n",
      "17092/17100 [============================>.] - ETA: 0s - loss: 0.2465 number of users skipped:  0\n",
      "17098/17100 [============================>.] - ETA: 0s - loss: 0.2465 number of users skipped:  0\n",
      "17100/17100 [==============================] - 357s 21ms/step - loss: 0.2465 - val_loss: 0.2901\n",
      "score L1:  0.880875\n",
      "Epoch 3/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.1809 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.1809 number of users skipped:  0\n",
      "17100/17100 [==============================] - 367s 21ms/step - loss: 0.1809 - val_loss: 0.2697\n",
      "score L1:  0.8855\n",
      "Epoch 4/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.1414 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.1415 number of users skipped:  0\n",
      "17100/17100 [==============================] - 342s 20ms/step - loss: 0.1415 - val_loss: 0.2596\n",
      "score L1:  0.894125\n",
      "Epoch 5/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.1130 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.1131 number of users skipped:  0\n",
      "17100/17100 [==============================] - 374s 22ms/step - loss: 0.1131 - val_loss: 0.2417\n",
      "score L1:  0.902125\n",
      "Epoch 6/12\n",
      "17092/17100 [============================>.] - ETA: 0s - loss: 0.0936 number of users skipped:  0\n",
      "17098/17100 [============================>.] - ETA: 0s - loss: 0.0936 number of users skipped:  0\n",
      "17100/17100 [==============================] - 397s 23ms/step - loss: 0.0936 - val_loss: 0.2368\n",
      "score L1:  0.90425\n",
      "Epoch 7/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.0803 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.0804 number of users skipped:  0\n",
      "17100/17100 [==============================] - 398s 23ms/step - loss: 0.0804 - val_loss: 0.2354\n",
      "score L1:  0.901375\n",
      "Epoch 8/12\n",
      "17092/17100 [============================>.] - ETA: 0s - loss: 0.0709 number of users skipped:  0\n",
      "17098/17100 [============================>.] - ETA: 0s - loss: 0.0709 number of users skipped:  0\n",
      "17100/17100 [==============================] - 398s 23ms/step - loss: 0.0709 - val_loss: 0.2375\n",
      "score L1:  0.904875\n",
      "Epoch 9/12\n",
      "17091/17100 [============================>.] - ETA: 0s - loss: 0.0634 number of users skipped:  0\n",
      "17097/17100 [============================>.] - ETA: 0s - loss: 0.0635 number of users skipped:  0\n",
      "17100/17100 [==============================] - 398s 23ms/step - loss: 0.0635 - val_loss: 0.2330\n",
      "score L1:  0.906375\n",
      "Epoch 10/12\n",
      "17092/17100 [============================>.] - ETA: 0s - loss: 0.0575 number of users skipped:  0\n",
      "17098/17100 [============================>.] - ETA: 0s - loss: 0.0575 number of users skipped:  0\n",
      "17100/17100 [==============================] - 398s 23ms/step - loss: 0.0575 - val_loss: 0.2281\n",
      "score L1:  0.907\n",
      "Epoch 11/12\n",
      "17092/17100 [============================>.] - ETA: 0s - loss: 0.0525 number of users skipped:  0\n",
      "17098/17100 [============================>.] - ETA: 0s - loss: 0.0525 number of users skipped:  0\n",
      "17100/17100 [==============================] - 398s 23ms/step - loss: 0.0525 - val_loss: 0.2377\n",
      "score L1:  0.909875\n",
      "Epoch 12/12\n",
      "17090/17100 [============================>.] - ETA: 0s - loss: 0.0491 number of users skipped:  0\n",
      "17099/17100 [============================>.] - ETA: 0s - loss: 0.0492 number of users skipped:  0\n",
      "17100/17100 [==============================] - 397s 23ms/step - loss: 0.0492 - val_loss: 0.2341\n",
      "score L1:  0.906625\n",
      "training complete!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "generator model: draws samples from dataobj automatically\n",
    "define test_set manually from test set above for re-identification evaluation on hold-out\n",
    "\"\"\"\n",
    "\n",
    "my_tlrnn.train_generator(run_name=run_name, alpha=alpha, test_set=dataobj_test.triplet_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is how the model can be used to embedd a single sequence:\n",
    "\n",
    "# take single sequence out of test data set for example:\n",
    "single_seq = dataobj_test.data_list[0][0]\n",
    "\n",
    "if len(single_seq[:,0]) > my_tlrnn.sequence_len_max:\n",
    "    raise Exception(\"Input sequence is longer than seq-len-max. Shorten the sequence and repeat.\")\n",
    "\n",
    "# zero-padding for the sequence to bring it to the nominal sequence length\n",
    "# first dim is important, but for the single sequence case just 1\n",
    "seq = np.zeros((1, dataobj.sequence_len_max, my_tlrnn.cov_num), dtype='int32')\n",
    "seq[0, :len(single_seq[:,0]), :] = single_seq[:, :]\n",
    "\n",
    "# translate sequence into embedding vector with the trained tl_rnn model\n",
    "# model requires list of features\n",
    "single_embedd = my_tlrnn.model_pred.predict([seq[:, :, (i):(i+1)] for i in list(range(my_tlrnn.cov_num))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12670484  0.01557162  0.07797565 -0.05971112 -0.4180501   0.07241513\n",
      "   0.01756571 -0.15987296 -0.03673608  0.06769748 -0.00820197 -0.199908\n",
      "   0.06610264  0.03982965  0.11915255  0.03439994 -0.04824077 -0.08312859\n",
      "   0.02021081 -0.05244791 -0.04547481 -0.00708578 -0.03062822  0.04037116\n",
      "   0.05492684 -0.01577551 -0.11857747  0.12923999  0.00644235  0.15515679\n",
      "   0.08415571  0.06526165  0.04574451  0.06990935  0.06214139  0.04033377\n",
      "  -0.09651881 -0.07461414  0.14476398  0.11885166  0.04625008  0.09086294\n",
      "  -0.21538562  0.04252753 -0.01896777  0.06049617 -0.00833463 -0.0458732\n",
      "   0.02249925 -0.01404572  0.02047411  0.1769096  -0.04371607 -0.00906524\n",
      "   0.09585367 -0.03242134  0.02680962  0.02872449 -0.01748445 -0.22998078\n",
      "  -0.00192439 -0.12011491  0.03149238 -0.06925411 -0.0511024  -0.05010792\n",
      "   0.03355582 -0.03059018 -0.21594599 -0.09640229  0.00950254 -0.03980622\n",
      "  -0.09664448 -0.14582826  0.10067647  0.13773139 -0.07426036  0.07493069\n",
      "  -0.04835485 -0.08211111 -0.07620607  0.01913439 -0.01864645  0.08278316\n",
      "  -0.08577017  0.05639395  0.07965486  0.08857072  0.07303024 -0.01763658\n",
      "   0.11276806  0.05952914 -0.11156347  0.03639213 -0.05474902 -0.35279018\n",
      "  -0.03545677 -0.05152555 -0.11193595  0.01104952  0.12920143 -0.01752311\n",
      "   0.04333392  0.09298365  0.057286   -0.03784683 -0.0142181   0.1010045\n",
      "   0.22087975  0.02853243 -0.12274837 -0.08944438  0.02695159 -0.11499062\n",
      "   0.25232843  0.15405868  0.03833143  0.00682417 -0.08600619  0.05299823\n",
      "   0.02806005 -0.00714098  0.05332369  0.03579568  0.14972223 -0.0262747\n",
      "   0.00688085 -0.04978377]]\n"
     ]
    }
   ],
   "source": [
    "print(single_embedd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not happy with the accuracy, tune hyperparameters for better results. Increasing alpha can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF SCRIPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gpu_env] *",
   "language": "python",
   "name": "conda-env-gpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
